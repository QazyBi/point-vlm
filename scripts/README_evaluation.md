# Point-VLM Evaluation Artifacts

This document describes the evaluation artifacts generated by the Point-VLM evaluation script.

## Directory Structure

When you run the evaluation, it creates the following directory structure:

```
evaluation_artifacts/
└── eval_YYYYMMDD_HHMMSS/
    ├── visualizations/          # Generated visualization images
    ├── metrics/                 # Detailed metrics and results
    ├── logs/                    # Evaluation logs
    └── reports/                 # Evaluation reports
```

## Artifacts Description

### 1. Visualizations (`visualizations/`)

- **Format**: PNG images (300 DPI)
- **Content**: Side-by-side comparison of ground truth vs predicted points
- **Naming**: `sample_XXXX_label.png` (e.g., `sample_0001_car.png`)
- **Legend**: Blue dashed circles = Ground Truth, Red X markers = Predictions

### 2. Metrics (`metrics/`)

#### `detailed_results.json`
Complete evaluation results including:
- Model information
- Dataset information
- Individual prediction results
- Per-sample metrics

#### `metrics_summary.csv`
CSV file with per-sample metrics:
- `distance_error`: Euclidean distance between GT and predicted points
- `normalized_distance_error`: Distance normalized by image diagonal
- `mae_x`, `mae_y`: Mean Absolute Error in X and Y coordinates
- `mse_x`, `mse_y`: Mean Squared Error in X and Y coordinates
- `success_rate`: Percentage of points within 5% of image diagonal
- `num_gt_points`, `num_pred_points`: Number of ground truth and predicted points

#### `overall_metrics.json`
Aggregated metrics across all samples:
- Mean, median, and standard deviation of distance errors
- Overall success rate
- Sample counts

### 3. Logs (`logs/`)

#### `evaluation.log`
Detailed execution log including:
- Model loading progress
- Sample processing status
- Error messages and warnings
- Final summary statistics

### 4. Reports (`reports/`)

#### `evaluation_report.md`
Human-readable evaluation report with:
- Model and dataset information
- Overall performance metrics
- Links to all artifacts

## Running the Evaluation

### Prerequisites

Install the required dependencies:
```bash
pip install -r scripts/requirements_eval.txt
```

### Basic Usage

Run the evaluation script:
```bash
cd scripts
python run_evaluation.py
```

Or run the evaluation directly:
```bash
cd scripts
python evaluate.py
```

### Configuration

You can modify the evaluation parameters in the `Config` class in `evaluate.py`:

- `NUM_SAMPLES`: Number of samples to evaluate (default: 100,000)
- `ARTIFACTS_DIR`: Base directory for artifacts (default: `./evaluation_artifacts`)
- Visualization and metrics sample counts

## Metrics Interpretation

### Distance Error
- **Lower is better**
- Measured in pixels
- Normalized by image diagonal for scale-invariant comparison

### Success Rate
- **Higher is better**
- Percentage of predictions within 5% of image diagonal
- Threshold can be adjusted in `calculate_metrics()` function

### MAE/MSE
- **Lower is better**
- Mean Absolute Error and Mean Squared Error in X/Y coordinates
- Useful for understanding directional bias

## Example Output

After running the evaluation, you'll see output like:

```
2024-01-15 10:30:00 - INFO - Starting comprehensive evaluation...
2024-01-15 10:30:01 - INFO - Running evaluation on 1000 validation samples...
2024-01-15 10:30:01 - INFO - Running visualization on 10 samples...
2024-01-15 10:30:05 - INFO - Processing sample 1/10
2024-01-15 10:30:05 - INFO - Saved visualization: evaluation_artifacts/eval_20240115_103000/visualizations/sample_0001_car.png
...
2024-01-15 10:35:00 - INFO - Evaluation complete! Results saved to: evaluation_artifacts/eval_20240115_103000
2024-01-15 10:35:00 - INFO - === EVALUATION SUMMARY ===
2024-01-15 10:35:00 - INFO - Total samples evaluated: 100
2024-01-15 10:35:00 - INFO - Valid predictions: 95
2024-01-15 10:35:00 - INFO - Mean distance error: 45.2341
2024-01-15 10:35:00 - INFO - Mean success rate: 0.7234
```