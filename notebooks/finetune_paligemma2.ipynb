{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning PaliGemma-2 for Visual Pointing Tasks\n",
    "\n",
    "This notebook walks through the process of fine-tuning the google/paligemma2-3b-pt-224 model to identify objects in an image and specify their coordinates.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Load and prepare a large-scale image dataset (allenai/pixmo-points).\n",
    "- Set up a powerful multimodal model (PaliGemma-2) for fine-tuning.\n",
    "- Use memory-efficient training techniques like QLoRA.\n",
    "- Define a custom data collator to format image, text, and point data.\n",
    "- Train the model using the Hugging Face Trainer.\n",
    "- Run inference and visually evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import hashlib\n",
    "import requests\n",
    "from urllib.parse import urlparse, unquote, parse_qs\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from matplotlib.patches import Circle\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration\n",
    "We define a `Config` class to hold all our hyperparameters and settings in one place. This makes the code cleaner and easier to modify.\n",
    "\n",
    "**Key Parameters**:\n",
    "- **`MODEL_ID`**: We are using `google/paligemma2-3b-pt-224`, a powerful vision-language model.\n",
    "- **`DATASET_ID`**: The `allenai/pixmo-points` dataset contains images with associated point coordinates for various objects. [1]\n",
    "- **`USE_QLORA`**: We set this to `True` to leverage Quantized Low-Rank Adaptation (QLoRA). This technique significantly reduces memory usage by quantizing the model to 4-bit and then attaching small, trainable \"LoRA\" adapters. [2] This allows us to fine-tune a large 3-billion parameter model on a single GPU.\n",
    "- **`LORA_TARGET_MODULES`**: This is a crucial parameter for LoRA. It specifies which layers of the model we will attach the trainable adapters to. Targeting the attention projection layers (`q_proj`, `k_proj`, `v_proj`, `o_proj`) and MLP layers (`gate_proj`, `up_proj`, `down_proj`) is a common and effective strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    SEED = 42 # Use a seed for reproducibility of the random shuffle\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # MODEL_ID = \"HuggingFaceTB/SmolVLM-Base\"\n",
    "    MODEL_ID =\"google/paligemma2-3b-pt-224\"\n",
    "\n",
    "    # Data Params\n",
    "    DATASET_ID = \"allenai/pixmo-points\"\n",
    "    NUM_SAMPLES = 100000\n",
    "    IMAGE_CACHE_DIR = Path(\"./image_cache\")\n",
    "    IMAGE_CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    # Training strategy\n",
    "    # Set to True to use QLoRA for memory-efficient fine-tuning\n",
    "    USE_QLORA = True\n",
    "\n",
    "    # QLoRA-specific configurations\n",
    "    LORA_R = 8\n",
    "    LORA_ALPHA = 8\n",
    "    LORA_DROPOUT = 0.1\n",
    "    # Add all linear layers of the model to the target modules\n",
    "    LORA_TARGET_MODULES = [\"o_proj\", \"k_proj\", \"q_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "    # Training arguments\n",
    "    OUTPUT_DIR = \"output/paligemma2-qlora-finetuned\"\n",
    "    NUM_TRAIN_EPOCHS = 3\n",
    "    PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4 # Increases effective batch size\n",
    "    LEARNING_RATE = 1e-4\n",
    "    OPTIM = \"paged_adamw_8bit\" # Recommended for QLoRA\n",
    "\n",
    "    # Name for the final model on the Hub\n",
    "    # HUB_MODEL_ID = \"SmolVLM-finetuned-pixmo-points\"\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Loading and Preparation\n",
    "Here, we load the model and prepare it for training.\n",
    "\n",
    "1.  **`PaliGemmaProcessor`**: This is a utility that handles both text tokenization and image preprocessing, ensuring the inputs are in the exact format the PaliGemma model expects.\n",
    "2.  **`BitsAndBytesConfig`**: This configures the 4-bit quantization for QLoRA. `load_in_4bit=True` enables quantization, and `bnb_4bit_compute_dtype=torch.bfloat16` sets the data type for computations, which is crucial for maintaining performance on modern GPUs.\n",
    "3.  **`LoraConfig`**: This defines the parameters for the LoRA adapters.\n",
    "4.  **`PaliGemmaForConditionalGeneration.from_pretrained`**: We load the model, applying the quantization config directly. `device_map=\"auto\"` intelligently distributes the model across available GPUs.\n",
    "5.  **Freezing Parameters**: A key step in parameter-efficient fine-tuning is to freeze the original model weights. We explicitly set `requires_grad = False` for the `vision_tower` and `multi_modal_projector`. The subsequent call to `get_peft_model` will ensure that only the LoRA adapter weights are trainable.\n",
    "6.  **`get_peft_model`**: This function from the `peft` library wraps our base model and injects the LoRA adapters according to our `lora_config`.\n",
    "\n",
    "The output shows that we have ~10.4 million trainable parameters, which is only a tiny fraction of the original 3 billion parameters. This is the magic of QLoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/torch/cuda/__init__.py:235: UserWarning: \n",
      "NVIDIA H100 80GB HBM3 with CUDA capability sm_90 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_35 sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_89 compute_89.\n",
      "If you want to use the NVIDIA H100 80GB HBM3 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10383360, 3044118768)\n",
      "The model as is is holding: 4.49 of GPU RAM\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "processor = PaliGemmaProcessor.from_pretrained(config.MODEL_ID)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=config.LORA_R,\n",
    "    lora_alpha=config.LORA_ALPHA,\n",
    "    lora_dropout=config.LORA_DROPOUT,\n",
    "    target_modules=config.LORA_TARGET_MODULES,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    use_dora=False if config.USE_QLORA else True,\n",
    ")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    config.MODEL_ID,\n",
    "    quantization_config=bnb_config if config.USE_QLORA else None,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "# model.add_adapter(lora_config)\n",
    "# model.enable_adapters()\n",
    "model.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model = model.to(config.DEVICE)\n",
    "\n",
    "for param in model.vision_tower.parameters():\n",
    "      param.requires_grad = False\n",
    "for param in model.multi_modal_projector.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "print(model.get_nb_trainable_parameters())\n",
    "\n",
    "peak_mem = torch.cuda.max_memory_allocated()\n",
    "print(f\"The model as is is holding: {peak_mem / 1024**3:.2f} of GPU RAM\")\n",
    "\n",
    "TORCH_DTYPE = model.dtype\n",
    "print(TORCH_DTYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Preparation\n",
    "The dataset contains URLs to images. We need to download them, cache them locally to speed up training, and perform some cleaning.\n",
    "\n",
    "**Steps**:\n",
    "1.  **Load Dataset**: Load the `train` split from the Hugging Face Hub.\n",
    "2.  **Subset**: We take a random subset of 100,000 samples for faster processing.\n",
    "3.  **Download and Cache**: The `download_and_cache_image` function is mapped across the dataset. It downloads each image, saves it to the `IMAGE_CACHE_DIR`, and adds the local file path to the dataset. Using `num_proc` speeds this up significantly.\n",
    "4.  **Filter**: We filter out examples where the download failed, where there are no points, or where the image is invalid (e.g., corrupted or has an incorrect number of channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the Hugging Face Hub\n",
    "ds = load_dataset(config.DATASET_ID, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a random subset of 100000 samples.\n"
     ]
    }
   ],
   "source": [
    "# Check if the dataset is larger than the number of samples we want\n",
    "if len(ds) > config.NUM_SAMPLES:\n",
    "    # Shuffle the dataset and select the first NUM_SAMPLES\n",
    "    small_ds = ds.shuffle(seed=config.SEED).select(range(config.NUM_SAMPLES))\n",
    "    print(f\"Created a random subset of {len(small_ds)} samples.\")\n",
    "else:\n",
    "    # If the dataset is smaller than our target, just use the whole thing\n",
    "    small_ds = ds\n",
    "    print(f\"The full dataset ({len(ds)} samples) is smaller than {config.NUM_SAMPLES}, so using the full dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to download and cache images...\n",
      "Image caching complete.\n",
      "Successfully downloaded 84561 out of 100000 images.\n",
      "71705/84561 samples after filtering\n"
     ]
    }
   ],
   "source": [
    "def url2path(url):\n",
    "    try:\n",
    "        # Generate a hash of the URL for uniqueness\n",
    "        url_hash = hashlib.md5(url.encode()).hexdigest()[:8]\n",
    "        \n",
    "        # Parse URL and get base filename\n",
    "        parsed_url = urlparse(url)\n",
    "        base_filename = os.path.basename(parsed_url.path)\n",
    "        base_filename = unquote(base_filename)\n",
    "        \n",
    "        # Remove query parameters and clean filename\n",
    "        base_filename = base_filename.split('?')[0]\n",
    "        base_filename = re.sub(r'[^A-Za-z0-9._-]', '_', base_filename)\n",
    "        \n",
    "        # Get extension from URL or default to jpg\n",
    "        ext = os.path.splitext(base_filename)[1].lower()\n",
    "        if not ext or ext not in ['.jpg', '.jpeg', '.png', '.webp', '.gif']:\n",
    "            # Check query parameters for format\n",
    "            query_params = parse_qs(parsed_url.query)\n",
    "            if 'format' in query_params:\n",
    "                format_param = query_params['format'][0].lower()\n",
    "                if format_param in ['jpg', 'jpeg', 'png', 'webp', 'gif']:\n",
    "                    ext = f'.{format_param}'\n",
    "            else:\n",
    "                ext = '.jpg'\n",
    "        \n",
    "        filename_length = len(base_filename)\n",
    "\n",
    "        # Create final filename\n",
    "        if not base_filename or base_filename == 'image' or filename_length > 200:\n",
    "            filename = f'image_{url_hash}{ext}'\n",
    "        else:\n",
    "            base_filename = os.path.splitext(base_filename)[0]\n",
    "            filename = f'{base_filename}_{url_hash}{ext}'\n",
    "\n",
    "        local_path = config.IMAGE_CACHE_DIR / filename\n",
    "        \n",
    "        # Ensure the path is not a directory\n",
    "        if local_path.is_dir():\n",
    "            local_path = local_path.with_name(f'{local_path.name}_file{ext}')\n",
    "            \n",
    "        return local_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {str(e)}\")\n",
    "        # Fallback to a safe filename with hash\n",
    "        return config.IMAGE_CACHE_DIR / f'image_{hashlib.md5(url.encode()).hexdigest()}.jpg'\n",
    "\n",
    "\n",
    "def download_and_cache_image(example):\n",
    "    \"\"\"\n",
    "    Downloads the image from its URL, saves it to a local cache,\n",
    "    and returns a dictionary with the new local path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a unique, safe filename from the image URL\n",
    "        image_url = example[\"image_url\"]\n",
    "        # Use the last part of the URL as a filename, which is usually unique\n",
    "        local_path = url2path(image_url)\n",
    "\n",
    "        # Only download if the file doesn't already exist in the cache\n",
    "        if not local_path.exists():\n",
    "            response = requests.get(image_url, timeout=10, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Save the image to the local cache\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        \n",
    "        # Return the new column\n",
    "        return {\"local_image_path\": str(local_path), \"download_status\": \"ok\"}\n",
    "    \n",
    "    except Exception as e:\n",
    "        # If download fails, mark it so we can filter it out later\n",
    "        return {\"local_image_path\": None, \"download_status\": \"error\"}\n",
    "\n",
    "# Use `map` to apply the download function to the entire dataset.\n",
    "# We set `num_proc` to a higher value to parallelize the download process.\n",
    "# This will show a progress bar.\n",
    "print(\"Starting to download and cache images...\")\n",
    "ds_with_local_images = small_ds.map(\n",
    "    download_and_cache_image,\n",
    "    num_proc=os.cpu_count()  # Use half of the available CPU cores\n",
    ")\n",
    "\n",
    "# Filter out any examples that failed to download\n",
    "original_rows = len(ds_with_local_images)\n",
    "ds_ready = ds_with_local_images.filter(lambda x: x[\"download_status\"] == \"ok\")\n",
    "filtered_rows = len(ds_ready)\n",
    "\n",
    "print(\"Image caching complete.\")\n",
    "print(f\"Successfully downloaded {filtered_rows} out of {original_rows} images.\")\n",
    "\n",
    "# filter samples that have no points\n",
    "ds_ready = ds_ready.filter(lambda x: len(x[\"points\"]) > 0)\n",
    "print(f\"{len(ds_ready)}/{filtered_rows} samples after filtering\")\n",
    "\n",
    "\n",
    "def check_image(x):\n",
    "    try:\n",
    "        img = Image.open(x[\"local_image_path\"]).convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "        if img.shape[2] != 3 or img.shape[1] == 1:\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# This runs the function in parallel across 4 separate processes\n",
    "# It's often faster for CPU-bound or local disk I/O tasks.\n",
    "ds_ready = ds_ready.filter(check_image, num_proc=16)\n",
    "ds_ready = ds_ready.remove_columns(['image_url', 'image_sha256', 'count', 'collection_method', 'download_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70914"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Formatting Prompts and Labels\n",
    "The model needs to be trained in a chat-like format. We define:\n",
    "- **`instruction_list`**: A list of various ways to ask the model to point to an object. During training, we randomly pick one to make the model robust to different phrasings.\n",
    "- **`points_to_text`**: This function converts a list of `(x, y)` coordinates into the target string format (`<point ...>` or `<points ...>`). This will be our label.\n",
    "- **`text_to_points`**: The inverse function, useful for parsing the model's output during inference and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_list = [\n",
    "        \"Point to {label}\\nPlease say 'This isn't in the image.' if it is not in the image.\",\n",
    "        \"Point to all occurrences of \\\"{label}\\\"\",\n",
    "        \"Point to any {label} in the image\",\n",
    "        \"Point to any {label} in the image.\",\n",
    "        \"Point: Where are the {label}\",\n",
    "        \"Show me where the {label} are\",\n",
    "        \"Can you show me where the {label} are?\",\n",
    "        \"Show me where the {label} are\",\n",
    "        \"Show me where a {label} is\",\n",
    "        \"Show me where a {label} is.\",\n",
    "        \"If there are any {label} in the image? Show me where they are.\",\n",
    "        \"Where are the {label}?\",\n",
    "        \"Generate a list of points showing where the {label} are.\",\n",
    "        \"Find the \\\"{label}\\\".\",\n",
    "        \"Find a \\\"{label}\\\".\",\n",
    "        \"Locate all {label}.\",\n",
    "        \"Locate an {label}.\",\n",
    "        \"Locate a {label}.\",\n",
    "        \"Locate every {label}.\",\n",
    "        \"Locate {label}.\",\n",
    "        \"Locate the {label}.\",\n",
    "        \"Object: {label}\\nInstruction: Point to the object.\",\n",
    "        \"find {label}\",\n",
    "        \"find {label}.\",\n",
    "        \"Point to every {label}\",\n",
    "        \"find any {label} in the picture\",\n",
    "        \"Find the {label}\",\n",
    "        \"Find any {label}\",\n",
    "        \"Point to a {label}\",\n",
    "        \"Point to an {label}\",\n",
    "        \"Look for {label} in the image and show me where they are.\",\n",
    "        \"Help me find an object in the image by pointing to them.\\nObject: {label}.\",\n",
    "        \"I am looking for {label}, where can they be found in the image?\",\n",
    "        \"Can you see any {label} in the image? Point to them.\",\n",
    "        \"Point out each {label} in the image.\",\n",
    "        \"Point out every {label} in the image.\",\n",
    "        \"Point to the {label} in the image.\",\n",
    "        \"Locate each {label} in the image.\",\n",
    "        \"Can you point out all {label} in this image?\",\n",
    "        \"Please find {label} and show me where they are.\",\n",
    "        \"If there are any {label} present, indicate their positions.\",\n",
    "        \"If there is a {label} present, indicate its positions.\",\n",
    "        \"show me all visible {label}\",\n",
    "]\n",
    "\n",
    "\n",
    "def points_to_text(points, label_text, alt_text):\n",
    "    if len(points) == 1:\n",
    "        x_str, y_str = points[0]\n",
    "        return f\"<point x=\\\"{x_str}\\\" y=\\\"{y_str}\\\" alt=\\\"{alt_text}\\\">{label_text}</point>\"\n",
    "    point_text = []\n",
    "    for ix, (x, y) in enumerate(points, start=1):\n",
    "        point_text.append(f\"x{ix}=\\\"{x}\\\"\")\n",
    "        point_text.append(f\"y{ix}=\\\"{y}\\\"\")\n",
    "    point_text = \" \".join(point_text)\n",
    "    return f\"<points {point_text} alt=\\\"{alt_text}\\\">{label_text}</points>\"\n",
    "\n",
    "\n",
    "def text_to_points(text):\n",
    "    # Single point pattern with flexible spaces and non-greedy label match\n",
    "    single_point_pattern = r'^<point\\s+x=\"([^\"]+)\"\\s+y=\"([^\"]+)\"\\s+alt=\"([^\"]+)\">(.*?)</point>$'\n",
    "    m = re.match(single_point_pattern, text, re.DOTALL)\n",
    "    if m:\n",
    "        x_str, y_str, alt_text, label_text = m.groups()\n",
    "        points = [(x_str, y_str)]\n",
    "        return points, label_text, alt_text\n",
    "\n",
    "    # Multiple points pattern\n",
    "    if text.startswith(\"<points\"):\n",
    "        alt_match = re.search(r'alt=\"([^\"]+)\"', text)\n",
    "        label_match = re.search(r'>(.*?)</points>', text, re.DOTALL)\n",
    "        if not alt_match or not label_match:\n",
    "            raise ValueError(\"Invalid format for multiple points\")\n",
    "        alt_text = alt_match.group(1)\n",
    "        label_text = label_match.group(1)\n",
    "\n",
    "        x_matches = re.findall(r'x(\\d+)=\"([^\"]+)\"', text)\n",
    "        y_matches = re.findall(r'y(\\d+)=\"([^\"]+)\"', text)\n",
    "\n",
    "        x_dict = {int(idx): val for idx, val in x_matches}\n",
    "        y_dict = {int(idx): val for idx, val in y_matches}\n",
    "\n",
    "        points = []\n",
    "        for i in sorted(x_dict.keys()):\n",
    "            if i in y_dict:\n",
    "                points.append((x_dict[i], y_dict[i]))\n",
    "            else:\n",
    "                raise ValueError(f\"Missing y{i} value\")\n",
    "\n",
    "        return points, label_text, alt_text\n",
    "\n",
    "    raise ValueError(\"Input string does not start with <point> or <points>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data Collator\n",
    "The `collate_fn` is a critical piece. The `Trainer` takes individual samples from the dataset and groups them into a batch. This function defines *how* that happens.\n",
    "\n",
    "For each sample in the batch, it:\n",
    "1.  Opens the cached image and resizes it to 224x224, the expected input size for the model's vision tower.\n",
    "2.  Randomly selects an instruction from our `instruction_list` and formats it with the sample's label.\n",
    "3.  Constructs the target output string using `points_to_text`.\n",
    "4.  Finally, it uses the `PaliGemmaProcessor` to convert the batch of images, prefixes (instructions), and suffixes (target point strings) into a dictionary of tensors (`input_ids`, `attention_mask`, `pixel_values`, `labels`) ready to be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_batch keys:\n",
      "\t input_ids torch.Size([2, 664])\n",
      "\t token_type_ids torch.Size([2, 664])\n",
      "\t attention_mask torch.Size([2, 664])\n",
      "\t pixel_values torch.Size([2, 3, 224, 224])\n",
      "\t labels torch.Size([2, 664])\n",
      "tensor([[     0,      0,      0,  ...,    727,   3371, 235313],\n",
      "        [257152, 257152, 257152,  ...,    727,   9580, 235313]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def debug_vis_img_point(img, point_str):\n",
    "    img = np.array(img)\n",
    "    plt.imshow(img)\n",
    "    points, label, alt_text = text_to_points(point_str)\n",
    "    for p in points:\n",
    "        x, y = list(map(float, p))\n",
    "        h, w = img.shape[:2]\n",
    "        circle = Circle(((x / 100) * h, (y / 100) * w), 5, fill=False, edgecolor='red', linewidth=2)\n",
    "        ax = plt.gca()\n",
    "        ax.add_patch(circle)\n",
    "    ax.set_title(f\"{label}\")\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    print(point_str)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    prefixes = []  # what will be sent along with image\n",
    "    suffixes = []  # what's expected\n",
    "\n",
    "    for sample in batch:\n",
    "        # prepare inputs\n",
    "        try:\n",
    "            image = Image.open(sample[\"local_image_path\"]).convert(\"RGB\")\n",
    "\n",
    "            # make resize here\n",
    "            image = image.resize((224, 224))\n",
    "            images.append(image)\n",
    "        except (IOError, FileNotFoundError) as e:\n",
    "            print(f\"Skipping sample due to image loading error from local cache: {e}\")\n",
    "            continue\n",
    "\n",
    "        instr = random.choice(instruction_list)\n",
    "        label = \"<image>\" + instr.format(label=sample['label'])\n",
    "        prefixes.append(label)\n",
    "\n",
    "        # prepare what's expected\n",
    "        w, h = image.size\n",
    "        points = np.stack([[p[\"x\"], p[\"y\"]] for p in sample[\"points\"]])\n",
    "        points_str = points_to_text(points, sample['label'], sample['label'])\n",
    "        suffixes.append(points_str)\n",
    "\n",
    "    # For Debug Uncomment when needed\n",
    "    # for i in range(2):\n",
    "    #     debug_vis_img_point(images[i], suffixes[i])\n",
    "\n",
    "    inputs = processor(\n",
    "        text=prefixes,\n",
    "        images=images,\n",
    "        suffix=suffixes,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(TORCH_DTYPE).to(config.DEVICE)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "sample_batch = collate_fn(ds_ready.select(range(2)))\n",
    "print(\"sample_batch keys:\")\n",
    "for k, v in sample_batch.items():\n",
    "    print(\"\\t\", k, v.shape)\n",
    "\n",
    "print(sample_batch[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training\n",
    "\n",
    "**Mistake Corrected**: The original code created a train/validation split (`train_val_split`) but then passed the entire `ds_ready` to the `Trainer`'s `train_dataset` argument. This means the model would not be evaluated on a hold-out set during training. I've corrected this by passing `train_ds` to `train_dataset` and `val_ds` to `eval_dataset`.\n",
    "\n",
    "1.  **`TrainingArguments`**: We configure the training process, setting the number of epochs, batch size, learning rate, and other parameters from our `Config` class. We enable `bf16=True` for mixed-precision training, which speeds up computation and reduces memory usage. `gradient_checkpointing=True` is another memory-saving technique.\n",
    "2.  **Train/Validation Split**: We split our prepared dataset into a 90% training set and a 10% validation set.\n",
    "3.  **`Trainer`**: We initialize the `Trainer` with our model, training arguments, datasets, and the custom `collate_fn`.\n",
    "4.  **`trainer.train()`**: This command starts the fine-tuning process. The `KeyboardInterrupt` in your original notebook indicates that this step was stopped manually. **You will need to run this cell and let it complete fully.** This will take a significant amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    num_train_epochs=config.NUM_TRAIN_EPOCHS,\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=config.PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=1e-6,\n",
    "    adam_beta2=0.999,\n",
    "    logging_steps=100,\n",
    "    optim=config.OPTIM,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=1,\n",
    "    output_dir=config.OUTPUT_DIR,\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,\n",
    "    # report_to=[\"tensorboard\"],\n",
    "    dataloader_pin_memory=False\n",
    ")\n",
    "\n",
    "train_val_split = ds_ready.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = train_val_split['train']\n",
    "val_ds = train_val_split['test']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=ds_ready,\n",
    "    data_collator=collate_fn,\n",
    "    args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1842' max='53184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1842/53184 46:12 < 21:29:34, 0.66 it/s, Epoch 0.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.428000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.249700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.227100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.200600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.151300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.162000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/home/jovyan/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:950: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/jovyan/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jovyan/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/transformers/trainer.py:2472\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2470\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2471\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2472\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[1;32m   2474\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/transformers/trainer.py:5131\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5131\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   5132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5133\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/accelerate/data_loader.py:577\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[0;32m--> 577\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m~/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# prepare inputs\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal_image_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# make resize here\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n",
      "File \u001b[0;32m~/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/PIL/Image.py:982\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    980\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 982\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/shares/SR006.nfs2/askarbek/point-vlm/.pixi/envs/default/lib/python3.10/site-packages/PIL/ImageFile.py:403\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_end()\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exclusive_fp \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_exclusive_fp_after_loading:\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m LOAD_TRUNCATED_IMAGES \u001b[38;5;129;01mand\u001b[39;00m err_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# still raised if decoder fails to return anything\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Save Final Model\n",
    "After training is complete, it's crucial to save your work. We save the trained model adapters and the processor. This allows us to easily load the fine-tuned model later for inference without having to repeat the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving fine-tuned model to {config.OUTPUT_DIR}\")\n",
    "trainer.save_model(config.OUTPUT_DIR)\n",
    "processor.save_pretrained(config.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Inference and Evaluation\n",
    "\n",
    "Now for the exciting part: let's see how well our model performs! This section was missing from the original notebook.\n",
    "\n",
    "**Inference Steps**:\n",
    "1.  **Load Model**: We load the base model again, but this time we also load the fine-tuned LoRA adapters from our output directory using `PeftModel.from_pretrained`.\n",
    "2.  **Prepare Inputs**: We'll take a sample from our validation set (`val_ds`). We format a prompt just like we did in training.\n",
    "3.  **Generate**: We use `model.generate()` to get the model's prediction. `max_new_tokens` controls the maximum length of the output.\n",
    "4.  **Decode and Visualize**: We decode the generated token IDs back into a string. We then use a helper function, `run_and_visualize`, to parse the predicted points and draw them on the original image, which provides an intuitive way to assess the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load the base model and processor from the saved directory\n",
    "print(f\"Loading model from {config.OUTPUT_DIR}\")\n",
    "base_model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    config.MODEL_ID,\n",
    "    quantization_config=bnb_config, # Must use the same quantization config\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "processor = PaliGemmaProcessor.from_pretrained(config.OUTPUT_DIR)\n",
    "model = PeftModel.from_pretrained(base_model, config.OUTPUT_DIR)\n",
    "model = model.to(config.DEVICE)\n",
    "model.eval() # Set the model to evaluation mode\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle\n",
    "\n",
    "def run_and_visualize(sample):\n",
    "    \"\"\"Runs inference on a single sample and visualizes the result.\"\"\"\n",
    "    image = Image.open(sample[\"local_image_path\"]).convert(\"RGB\")\n",
    "    label_text = sample['label']\n",
    "    \n",
    "    # Prepare the prompt\n",
    "    prompt = f\"<image>\\nPoint to {label_text}\"\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    \n",
    "    # Decode and parse\n",
    "    generated_text = processor.decode(generation[0], skip_special_tokens=True)\n",
    "    # We need to extract just the generated part (the <point> string)\n",
    "    prediction_str = generated_text.split(\"\\n\")[-1]\n",
    "    predicted_points, _, _ = text_to_points(prediction_str)\n",
    "\n",
    "    # Get ground truth points\n",
    "    gt_points = [(p['x'], p['y']) for p in sample[\"points\"]]\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(f\"Prompt: 'Point to {label_text}'\")\n",
    "\n",
    "    # Draw ground truth points (blue circles)\n",
    "    for x, y in gt_points:\n",
    "        h, w = image.height, image.width\n",
    "        circle = Circle((x * w, y * h), radius=8, fill=False, edgecolor='blue', linewidth=2, linestyle='--')\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "    # Draw predicted points (red 'X')\n",
    "    for x, y in predicted_points:\n",
    "        h, w = image.height, image.width\n",
    "        ax.scatter(x * w, y * h, marker='x', color='red', s=100, linewidths=3)\n",
    "\n",
    "    # Create legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [Line2D([0], [0], color='blue', marker='o', linestyle='--', label='Ground Truth', markersize=10, markerfacecolor='none'),\n",
    "                       Line2D([0], [0], marker='x', color='red', label='Prediction', markersize=10, linestyle='None')]\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Ground Truth:\", points_to_text(gt_points, label_text, label_text))\n",
    "    print(\"Prediction:\", prediction_str)\n",
    "\n",
    "# Run visualization on a few random samples from the validation set\n",
    "for i in range(5):\n",
    "    print(f\"--- Sample {i+1} ---\")\n",
    "    random_index = random.randint(0, len(val_ds) - 1)\n",
    "    run_and_visualize(val_ds[random_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation Metric:\n",
    "\n",
    "Visual inspection is good, but a numerical score is better for rigorous evaluation.\n",
    "\n",
    "**Object-based Pointing Accuracy (OPA)**. For a given prediction, if the distance to the nearest ground truth point is within a certain threshold (e.g., 15% of the image diagonal), it's considered a true positive. You could then calculate Precision, Recall, and F1-score over the entire validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def calculate_opa_metrics(predicted_points, gt_points, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Calculates Object-based Pointing Accuracy (OPA) metrics (Precision, Recall, F1).\n",
    "\n",
    "    A predicted point is considered a True Positive (TP) if its normalized distance\n",
    "    to the NEAREST ground truth point is within a given threshold.\n",
    "\n",
    "    Args:\n",
    "        predicted_points (list of tuples): List of (x, y) coordinates for predicted points.\n",
    "        gt_points (list of tuples): List of (x, y) coordinates for ground truth points.\n",
    "        threshold (float): The maximum normalized distance for a point to be considered a match.\n",
    "                           The distance is normalized by the image diagonal (sqrt(2)),\n",
    "                           so 0.05 corresponds to ~5% of the diagonal.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing precision, recall, and f1_score.\n",
    "    \"\"\"\n",
    "    # Handle edge case where there are no predictions.\n",
    "    if not predicted_points:\n",
    "        # If there are also no ground truth points, it's a perfect match.\n",
    "        if not gt_points:\n",
    "            return {'precision': 1.0, 'recall': 1.0, 'f1_score': 1.0}\n",
    "        # If there are ground truth points, we missed all of them.\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
    "\n",
    "    # Handle edge case where there are no ground truth points, but we predicted some.\n",
    "    # This is a pure hallucination scenario.\n",
    "    if not gt_points:\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
    "\n",
    "    predicted_array = np.array(predicted_points)\n",
    "    gt_array = np.array(gt_points)\n",
    "\n",
    "    # Calculate the pairwise distances between all predicted and ground truth points.\n",
    "    # The result is a matrix of shape (num_predicted, num_gt).\n",
    "    distances = cdist(predicted_array, gt_array)\n",
    "\n",
    "    # For each predicted point, find the distance to the nearest ground truth point.\n",
    "    min_distances = np.min(distances, axis=1)\n",
    "\n",
    "    # A prediction is a \"hit\" (True Positive) if its nearest GT point is within the threshold.\n",
    "    true_positives = np.sum(min_distances <= threshold)\n",
    "\n",
    "    # Calculate Precision and Recall\n",
    "    precision = true_positives / len(predicted_points)\n",
    "    recall = true_positives / len(gt_points)\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    if precision + recall == 0:\n",
    "        f1_score = 0.0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return {'precision': precision, 'recall': recall, 'f1_score': f1_score}\n",
    "\n",
    "\n",
    "# --- Main Evaluation Loop ---\n",
    "print(\"Starting quantitative evaluation over the entire validation set...\")\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "all_f1s = []\n",
    "\n",
    "# Use torch.no_grad() for the entire loop to save memory and speed up inference\n",
    "with torch.no_grad():\n",
    "    # Iterate over the validation dataset with a progress bar\n",
    "    for sample in tqdm(val_ds, desc=\"Evaluating\"):\n",
    "        image = Image.open(sample[\"local_image_path\"]).convert(\"RGB\")\n",
    "        label_text = sample['label']\n",
    "\n",
    "        # 1. Prepare the prompt for the model\n",
    "        prompt = f\"<image>\\nPoint to {label_text}\"\n",
    "        inputs = processor(\n",
    "            text=prompt,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(config.DEVICE)\n",
    "\n",
    "        # 2. Generate the model's output\n",
    "        generation = model.generate(**inputs, max_new_tokens=150, do_sample=False)\n",
    "        generated_text = processor.decode(generation[0], skip_special_tokens=True)\n",
    "\n",
    "        # 3. Parse the output to get predicted points\n",
    "        # The model's full output includes the prompt, so we extract the last part.\n",
    "        prediction_str = generated_text.split(\"\\n\")[-1].strip()\n",
    "        predicted_points, _, _ = text_to_points(prediction_str)\n",
    "\n",
    "        # 4. Get ground truth points\n",
    "        gt_points = [(p['x'], p['y']) for p in sample[\"points\"]]\n",
    "\n",
    "        # 5. Calculate metrics for this sample and store them\n",
    "        metrics = calculate_opa_metrics(predicted_points, gt_points, threshold=0.05)\n",
    "        all_precisions.append(metrics['precision'])\n",
    "        all_recalls.append(metrics['recall'])\n",
    "        all_f1s.append(metrics['f1_score'])\n",
    "\n",
    "# --- Report Final Scores ---\n",
    "mean_precision = np.mean(all_precisions)\n",
    "mean_recall = np.mean(all_recalls)\n",
    "mean_f1 = np.mean(all_f1s)\n",
    "\n",
    "print(\"\\n--- Quantitative Evaluation Results ---\")\n",
    "print(f\"Validation Set Size: {len(val_ds)}\")\n",
    "print(f\"Average Precision: {mean_precision:.4f}\")\n",
    "print(f\"Average Recall:    {mean_recall:.4f}\")\n",
    "print(f\"Average F1-Score:  {mean_f1:.4f}\")\n",
    "print(\"---------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointvlm",
   "language": "python",
   "name": "pointvlm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
